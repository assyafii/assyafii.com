<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>M. Luthfi As Syafii</title>
    <link>/</link>
    <description>Recent content on M. Luthfi As Syafii</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Nov 2018 15:14:39 +1000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deploy PortWorx storage in kubernetes</title>
      <link>/docs/deploy-portworx-storage-in-kubernetes/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/docs/deploy-portworx-storage-in-kubernetes/</guid>
      <description>Specification : Kubernetes, PortWorx, Storage
 PortWorx storage cluster Portworx by Pure Storage is a cloud native storage solution, provides a fully integrated solution for persistent storage, data protection, disaster recovery, data security, cross-cloud and data migrations, and automated capacity management for applications running on Kubernetes. If you see in each of documents, portworx have big IOPS &amp;amp; Bandwidth.
 Lab Topology    IP Address Nodes     10.</description>
    </item>
    
    <item>
      <title>5G Cloud Native Simulation with Open5Gs</title>
      <link>/docs/5g-cloud-native-simulation-with-open5gs/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/docs/5g-cloud-native-simulation-with-open5gs/</guid>
      <description>Specification : Kubernetes, HELM, Istio, Open5Gs, ROOK, CEPH, Rancher
 Lab Topology Create Namespaces for practices kubectl create ns open5gs Install Service mesh Istio (optional) curl -L https://istio.io/downloadIstio | sh - cd istio-1.12.1 export PATH=$PWD/bin:$PATH istioctl install --set profile=demo -y Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later:
kubectl label namespace open5gs istio-injection=enabled Install Addons packages
cd ~/istio-1.</description>
    </item>
    
    <item>
      <title>Deploy storage cluster ROOK with CEPH in Kubernetes</title>
      <link>/docs/deploy-storage-cluster-rook-with-ceph-in-kubernetes/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/docs/deploy-storage-cluster-rook-with-ceph-in-kubernetes/</guid>
      <description>Specification : Kubernetes, ROOK, CEPH
 Lab Topology You can check installation kubernetes cluster in previous documentation, https://assyafii.com/docs/install-kubernetes-cluster-multi-master-ha/  Storages nodes disks We use 3 disks extended (vdb, vdc, vdd) in each of storage-nodes, total 6 disks for rook cluster.
Detail disks Master node  Clone ROOK Project cd ~ git clone --single-branch --branch release-1.7 https://github.com/rook/rook.git Deploy the Rook Operator cd rook/cluster/examples/kubernetes/ceph kubectl create -f crds.yaml kubectl create -f common.</description>
    </item>
    
    <item>
      <title>Install Kubernetes cluster multi Master High Availability</title>
      <link>/docs/install-kubernetes-cluster-multi-master-ha/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/docs/install-kubernetes-cluster-multi-master-ha/</guid>
      <description>Specification : Calico, Containerd, Haproxy, Kubernetes v1.22.x
 Lab Topology First, prepare all VM All Nodes except LB Nodes  Set mapping hostname nano /etc/hosts Install packages containerd Load overlay and br_netfilter kernal modules.
cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter Set these system configurations for Kubernetes networking cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.</description>
    </item>
    
  </channel>
</rss>
